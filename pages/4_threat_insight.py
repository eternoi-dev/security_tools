import streamlit as st
import requests
from bs4 import BeautifulSoup
import json
import os
import re
from collections import Counter
from dotenv import load_dotenv
import pandas as pd
import altair as alt

load_dotenv()
webhook_url = os.getenv("SLACK_WEBHOOK_URL")

st.set_page_config(
    page_title="Threat Trend Insight",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

IGNORE_WORDS = {
    # æ–‡æ³•ãƒ»ä¸€èˆ¬çš„å˜èª
    "ã®", "ã«ãŠã‘ã‚‹", "ã«é–¢ã™ã‚‹", "ã«å¯¾ã™ã‚‹", "ãŠã‚ˆã³", "ã¾ãŸã¯",
    "è£½å“", "è¤‡æ•°", "ã‚µãƒ¼ãƒ“ã‚¹", "ã‚·ã‚¹ãƒ†ãƒ ", "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢",
    "æƒ…å ±", "ç®¡ç†", "å®Ÿè¡Œ", "å›é¿", "æ–¹æ³•", "ç¢ºèª", "è§£æ±º", "å¯¾å¿œ", "å¯¾ç­–",
    "ä½¿ç”¨", "å¯èƒ½", "å…¬é–‹", "æ”»æ’ƒ", "æ‚ªç”¨", "å½±éŸ¿", "ç™ºç”Ÿ", "ãƒ¦ãƒ¼ã‚¶ãƒ¼",
    "é€ä¿¡", "å—ä¿¡", "å‡¦ç†", "å‚ç…§", "ç”Ÿæˆ", "å®Œäº†", "æˆåŠŸ", "å¤±æ•—", "æ–°è¦",
    # å½¢å®¹è©ãƒ»å‰¯è©
    "ä»»æ„", "ä¸é©åˆ‡", "ä¸è¶³", "ä¸å…¨", "ä¸æ­£", "æœ‰åŠ¹", "ç„¡åŠ¹", "æ¬ å¦‚",
    "é‡è¦", "é‡å¤§", "è‡´å‘½çš„", "å±é™º", "å®‰å…¨", "æ­£å¸¸", "ç•°å¸¸", "è©³ç´°",
    "é©åˆ‡", "ä¸å‚™", "æ¤œè¨¼", "å®Ÿè£…", "è¨­å®š", "çŠ¶æ…‹", "å ´æ‰€",
    # Stop Words (English)
    "the", "a", "an", "in", "on", "at", "of", "for", "to", "with", "by", "from",
    "and", "or", "is", "are", "was", "were", "be", "has", "have", "it", "this", "that",
    "target", "attack", "remote", "arbitrary", "execution", "denial", "service",
    "improper", "insufficient", "missing", "validation",
    # ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰
    "è„†å¼±æ€§", "vulnerability", "vulnerabilities", "cve", "jvn",
    "server", "client", "user", "app", "ver", "version", "update",
    "ii", "iii", "iv", "v", "vi",
    "èªè¨¼", "ç­‰", "ãƒ™ãƒ³ãƒ€", "ãƒ™ãƒ³ãƒ€ãƒ¼"
}

PROTECTED_WORDS = {
    "æƒ…å ±æ¼ãˆã„": "InfoLeak",
    "æƒ…å ±æ¼æ´©": "InfoLeak",
    "ã‚¯ãƒ­ã‚¹ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒ†ã‚£ãƒ³ã‚°": "XSS",
    "ã‚¯ãƒ­ã‚¹ã‚µã‚¤ãƒˆãƒ»ã‚¹ã‚¯ãƒªãƒ—ãƒ†ã‚£ãƒ³ã‚°": "XSS",
    "ãƒãƒƒãƒ•ã‚¡ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼": "BufferOverflow",
    "ãƒãƒƒãƒ•ã‚¡ã‚ªãƒ¼ãƒãƒ¼ãƒªãƒ¼ãƒ‰": "BufferOverRead",
    "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«": "DirectoryTraversal",
    "ãƒªãƒ¢ãƒ¼ãƒˆã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ": "RCE",
    "ã‚µãƒ¼ãƒ“ã‚¹é‹ç”¨å¦¨å®³": "DoS",
    "SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³": "SQLi",
    "ã‚³ãƒãƒ³ãƒ‰ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³": "CmdInjection",
    "æ¨©é™æ˜‡æ ¼": "PrivEscalation",
    "ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ": "CodeExec"
}

def normalize_text_for_search(text):
    for jpn, eng in PROTECTED_WORDS.items():
        text = text.replace(jpn, f" {eng} ")
    return text

def fetch_rss_data(limit=None):
    target_url = "https://jvndb.jvn.jp/rss/jvndb.rdf"
    news_list = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124"
    }

    try:
        response = requests.get(target_url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, features="xml")
            items = soup.find_all("item")
            target_items = items[:limit] if limit else items
            
            for item in target_items:
                news_list.append({
                    "title": item.title.text,
                    "link": item.link.text,
                    "date": item.date.text[:10] if item.date else "---",
                    "description": item.description.text if item.description else ""
                })
        return news_list
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def extract_keywords(text):
    text = normalize_text_for_search(text)
    clean_text = re.sub(r'[!-/:-@[-`{-~ã€ã€‚ã-ã‚“]', ' ', text)
    words = clean_text.split()
    
    valid_words = []
    for w in words:
        w_lower = w.lower()
        if w_lower in IGNORE_WORDS: continue
        if len(w) < 2: continue
        if w.isdigit(): continue
        valid_words.append(w.capitalize())
    return valid_words

def highlight_title(text, keywords):
    for k in keywords:
        if not k: continue
        pattern = re.compile(re.escape(k), re.IGNORECASE)
        text = pattern.sub(f":orange[{k}]", text)
    return text

def analyze_dynamic_trends(news_data):
    all_words = []
    for item in news_data:
        words = extract_keywords(item['title'])
        all_words.extend(words)
    return Counter(all_words)

def send_daily_report(trend_text, detail_items):
    if not webhook_url: return False

    slack_blocks = [
        {"type": "header", "text": {"type": "plain_text", "text": "ğŸ“… æœ¬æ—¥ã®è„†å¼±æ€§è‡ªå‹•åˆ†æãƒ¬ãƒãƒ¼ãƒˆ", "emoji": True}},
        {"type": "section", "text": {"type": "mrkdwn", "text": trend_text}},
        {"type": "divider"},
        {"type": "section", "text": {"type": "mrkdwn", "text": "ğŸš¨ **é¸åˆ¥ã•ã‚ŒãŸé‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹**"}}
    ]

    for item in detail_items[:7]:
        slack_blocks.append({
            "type": "section",
            "text": {"type": "mrkdwn", "text": f"â€¢ <{item['link']}|{item['title']}>\n   date: {item['date']}"}
        })

    try:
        payload = {"blocks": slack_blocks}
        requests.post(webhook_url, data=json.dumps(payload), headers={'Content-Type': 'application/json'})
        return True
    except:
        return False

st.title("ğŸ“° Threat Trend Insight")
st.markdown("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªå‹•æŠ½å‡ºã—ã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã¨é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é¸å®šã‚’è¡Œã„ã¾ã™ã€‚")

# æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
with st.sidebar:
    st.header("âš™ï¸ æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š")
    
    # å›ºå®šç›£è¦–ãƒ¯ãƒ¼ãƒ‰è¨­å®š
    default_keywords = "VPN, Remote"
    keywords_input = st.text_input("âœ… å›ºå®šç›£è¦–ãƒ¯ãƒ¼ãƒ‰ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", default_keywords)
    user_watch_keywords = [k.strip() for k in keywords_input.split(",") if k.strip()]
    
    st.markdown("---")
    
    # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰è¨­å®š
    exclusion_input = st.text_input("â›” é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", "Linux, Beta")
    user_exclude_keywords = [k.strip() for k in exclusion_input.split(",") if k.strip()]
    st.caption("â€»ã“ã“ã«æŒ‡å®šã—ãŸå˜èªã‚’å«ã‚€è¨˜äº‹ã¯ã€ãƒªã‚¹ãƒˆã¨é€šçŸ¥ã‹ã‚‰å®Œå…¨ã«é™¤å¤–ã•ã‚Œã¾ã™ã€‚")

    st.markdown("---")
    
    # åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.button("ğŸ”„ åˆ†æã‚’é–‹å§‹", type="primary"):
        with st.spinner("æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è§£æä¸­..."):
            raw_data = fetch_rss_data()
            st.session_state["news_data"] = raw_data
            st.success("åˆ†æå®Œäº†")

if "news_data" in st.session_state and st.session_state["news_data"]:
    data = st.session_state["news_data"]
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    counter = analyze_dynamic_trends(data)
    top_trends = counter.most_common(10)
    top_10_keywords = [t[0] for t in top_trends]
    
    col_chart, col_list = st.columns([1, 2], gap="large")
    
    # --- å·¦ã‚«ãƒ©ãƒ : ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ ---
    with col_chart:

        # 1. ã‚µãƒãƒªãƒ¼
        m1, m2 = st.columns(2)
        m1.metric("åˆ†æè¨˜äº‹æ•°", f"{len(data)}ä»¶")
        m2.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", f"{len(counter)}èª")
        
        st.divider()

        # 2. ãƒˆãƒ¬ãƒ³ãƒ‰ã‚°ãƒ©ãƒ•
        st.subheader("ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ (Top 10)")
        if top_trends:
            df_chart = pd.DataFrame(top_trends, columns=['Keyword', 'Count'])
            
            chart = alt.Chart(df_chart).mark_bar().encode(
                x=alt.X('Keyword', 
                        axis=alt.Axis(labelAngle=-45, labelOverlap=False), 
                        sort='-y', 
                        title=None),
                y=alt.Y('Count', title="å‡ºç¾å›æ•°"),
                tooltip=['Keyword', 'Count'],
                color=alt.value('#FFAA00')
            )
            st.altair_chart(chart, use_container_width=True)
            
            trend_text = "*ã€è‡ªå‹•è§£æãƒˆãƒ¬ãƒ³ãƒ‰ã€‘*\n"
            for word, count in counter.most_common(3):
                trend_text += f"ğŸ”¥ *{word}* ({count}ä»¶)\n"
        else:
            st.info("ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚è¡¨ç¤ºã§ãã¾ã›ã‚“")
            trend_text = "ãƒˆãƒ¬ãƒ³ãƒ‰ãªã—"

        st.divider()

        # 3. é »å‡ºå˜èªãƒªã‚¹ãƒˆ
        st.subheader("ğŸ“‹ é »å‡ºå˜èªãƒªã‚¹ãƒˆ")
        st.caption("é™¤å¤–è¨­å®šã‚„ç›£è¦–ãƒ¯ãƒ¼ãƒ‰å€™è£œã®æ¤œè¨ã«ã”åˆ©ç”¨ãã ã•ã„ã€‚")
        
        if counter:
            df_freq = pd.DataFrame(counter.most_common(), columns=['å˜èª', 'å‡ºç¾å›æ•°'])
            st.dataframe(
                df_freq,
                use_container_width=True,
                height=400,
                hide_index=True
            )

    # --- å³ã‚«ãƒ©ãƒ : é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹é¸å®š ---
    with col_list:
        st.subheader("ğŸš¨ é¸åˆ¥ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹")

        all_filter_options = sorted(list(set(user_watch_keywords + top_10_keywords)))

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        active_filters = st.multiselect(
            "ğŸ” é©ç”¨ä¸­ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (å›ºå®šè¨­å®š + ãƒˆãƒ¬ãƒ³ãƒ‰Top10)",
            options=all_filter_options,
            default=user_watch_keywords
        )

        st.caption(f"ç¾åœ¨ã®ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶: {', '.join(active_filters)}")

        filtered_news = []
        for item in data:
            title_search = normalize_text_for_search(item['title'])
            desc_search = normalize_text_for_search(item['description'])
            target_text = (title_search + " " + desc_search).lower()
            
            if any(exc.lower() in target_text for exc in user_exclude_keywords):
                continue
                
            if active_filters:
                if any(k.lower() in target_text for k in active_filters):
                    filtered_news.append(item)
            else:
                pass

        # æ¤œç´¢çµæœ
        if filtered_news:
            st.success(f"è©²å½“ä»¶æ•°: {len(filtered_news)} ä»¶")
            with st.container(height=900):
                for item in filtered_news:
                    colored_title = highlight_title(item['title'], active_filters)
                    st.markdown(f"**{colored_title}**")
                    st.caption(f"ğŸ“… {item['date']} | [Link]({item['link']})")
                    st.divider()
        else:
            st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")

    # --- Slacké€ä¿¡ã‚¨ãƒªã‚¢ ---
    st.markdown("---")
    if st.button(f"ğŸ“¢ åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’Slackã¸ (æ–°ç€7ä»¶é€ä¿¡ / å…¨{len(filtered_news)}ä»¶)"):
        if send_daily_report(trend_text, filtered_news):
            st.toast("é€ä¿¡å®Œäº†ï¼", icon="ğŸš€")
            st.balloons()
        else:
            st.error("Slack Webhook URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

else:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œè‡ªå‹•åˆ†æã‚’é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

# ãƒ•ãƒƒã‚¿ãƒ¼ã‚¨ãƒªã‚¢
st.markdown("---")
st.caption("Â© 2025 Security Engineer Portfolio Demo | Created by eternoi-dev")